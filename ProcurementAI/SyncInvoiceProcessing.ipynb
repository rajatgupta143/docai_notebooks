{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade google-cloud-documentai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade simplejson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade googleapis-common-protos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import documentai_v1beta3 as documentai, bigquery\n",
    "import simplejson as json\n",
    "import proto\n",
    "from google.cloud import storage\n",
    "\n",
    "# [START documentai_process_document]\n",
    "\n",
    "# TODO(developer): Uncomment these variables before running the sample.\n",
    "# project_id= 'YOUR_PROJECT_ID';\n",
    "# location = 'YOUR_PROJECT_LOCATION'; // Format is 'us' or 'eu'\n",
    "# processor_id = 'YOUR_PROCESSOR_ID'; // Create processor in Cloud Console\n",
    "# bucket_name = '/path/to/bucket'\n",
    "# file_path = '/path/to/bucketFile/pdf';\n",
    "\n",
    "\n",
    "def process_document_sample(\n",
    "    project_id: str, location: str, processor_id: str, bucket_name: str, file_path: str\n",
    "):\n",
    "    # Instantiates a client\n",
    "    client = documentai.DocumentProcessorServiceClient()\n",
    "\n",
    "    # The full resource name of the processor, e.g.:\n",
    "    # projects/project-id/locations/location/processor/processor-id\n",
    "    # You must create new processors in the Cloud Console first\n",
    "    name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "\n",
    "    image_content = download_blob(bucket_name, file_path).download_as_bytes()\n",
    "\n",
    "    # Read the file into memory\n",
    "    document = {\"content\": image_content, \"mime_type\": \"application/pdf\"}\n",
    "\n",
    "    # Configure the process request\n",
    "    request = {\"name\": name, \"document\": document}\n",
    "\n",
    "    # Recognizes text entities in the PDF document\n",
    "    result = client.process_document(request=request)\n",
    "    \n",
    "    document = result.document\n",
    "    \n",
    "    entityDict={}\n",
    "    lineItem_text=\"\"\n",
    "\n",
    "    for entity in document.entities:\n",
    "        entity_type = entity.type_\n",
    "        #print (entity)\n",
    "        if(entity.normalized_value.text!=\"\"):\n",
    "            entity_text = entity.normalized_value.text\n",
    "        else:\n",
    "            entity_text = entity.mention_text\n",
    "        \n",
    "        # Placeholder code below to test whether the amount fields have strings with commas coming in. Converting them to floats for now.        \n",
    "        if(\"amount\" in entity_type and entity.normalized_value.text ==''):\n",
    "            entity_text = float(entity.mention_text.replace(',',''))\n",
    "            #print(\"Entity Type:{},entity_text:{}\".format(entity_type, entity_text))\n",
    "\n",
    "        if(entity_type==\"line_item\"):\n",
    "            lineItem_text =  lineItem_text+'{'\n",
    "            for prop in entity.properties:\n",
    "                pName=prop.type_[prop.type_.index(\"/\")+1:]\n",
    "                if(\"amount\" in pName and prop.normalized_value.text ==''):\n",
    "                    prop.mention_text = float(prop.mention_text.replace(',',''))\n",
    "                else:\n",
    "                    prop.mention_text =prop.normalized_value.text\n",
    "                lineItem_text = lineItem_text+ \"\\\"\"+pName +\"\\\"\"+\":\"+ \"[\\\"\"+prop.mention_text+ \"\\\"]\"+\",\"                                \n",
    "            lineItem_text = lineItem_text[0:lineItem_text.rindex(\",\")]\n",
    "            \n",
    "            lineItem_text = lineItem_text+'},'\n",
    "            #print(\"lineItem_text text before:{}\".format(lineItem_text))\n",
    "        if(entity_type!=\"line_item\"):\n",
    "            entityDict[entity_type]=entity_text    \n",
    "    \n",
    "    if(lineItem_text!=\"\"):\n",
    "        lineItem_text = lineItem_text[0:lineItem_text.rindex(\",\")]\n",
    "        lineItem_text = \"[\"+lineItem_text+\"]\"     \n",
    "        #Take out any special characters\n",
    "        lineItem_text = lineItem_text.replace('\\n', '')\n",
    "        lineItem_t = json.loads(lineItem_text)\n",
    "    \n",
    "        #print(\"Final Line Item:{}\".format(lineItem_t))\n",
    "        entityDict[\"line_item\"]=lineItem_t    \n",
    "    \n",
    "    #document_pages = document.pages\n",
    "\n",
    "    #Calling the WiteToBQ Method\n",
    "    #writeToBQ(entityDict)  \n",
    "\n",
    "# Extract shards from the text field\n",
    "def get_text(doc_element: dict, document: dict):\n",
    "    \"\"\"\n",
    "    Document AI identifies form fields by their offsets\n",
    "    in document text. This function converts offsets\n",
    "    to text snippets.\n",
    "    \"\"\"\n",
    "    response = \"\"\n",
    "    # If a text segment spans several lines, it will\n",
    "    # be stored in different text segments.\n",
    "    for segment in doc_element.text_anchor.text_segments:\n",
    "        #print(\"Segments: ---> \",doc_element.text_anchor.text_segments)\n",
    "        #print(\"Segment start index: ---> \",segment.start_index)\n",
    "        start_index = (\n",
    "            int(segment.start_index)            \n",
    "            if segment in doc_element.text_anchor.text_segments else 0\n",
    "            #if segment.start_index in doc_element.text_anchor.text_segments else 0\n",
    "        )\n",
    "        end_index = int(segment.end_index)\n",
    "        response += document.text[start_index:end_index]   \n",
    "        #print (\"Start Index:{}, End Index:{}\".format(start_index, end_index))\n",
    "    #print (\"returning text seg resp: {}\".format(response))\n",
    "    return response\n",
    "\n",
    "def download_blob(bucket_name, source_blob_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "\n",
    "    return blob\n",
    "    \n",
    "# [END documentai_process_document]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
